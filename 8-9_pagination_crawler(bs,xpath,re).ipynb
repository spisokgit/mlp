{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "# web_url에 원하는 웹의 URL을 넣어주시면 됩니다.\n",
    "with urllib.request.urlopen(web_url) as response:\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    \n",
    "import requests\n",
    "\n",
    "# web_url에 원하는 웹의 URL을 넣어주시면 됩니다.\n",
    ">>> r = requests.get(web_url)\n",
    ">>> r.status_code\n",
    "200\n",
    ">>> r.headers['content-type']\n",
    "'text/html; charset=UTF-8'\n",
    ">>> r.encoding\n",
    "'UTF-8'\n",
    ">>> r.text\n",
    "<!DOCTYPE html>\n",
    "<html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BeautifulSoup 메소드\n",
    "1. find()가장 첫번째 태그, find_all() 모든 태그\n",
    "\n",
    "    with open(\"example.html\") as fp:\n",
    "        soup = BeautifulSoup(fp, 'html.parser')\n",
    "        all_divs = soup.find_all(\"div\")\n",
    "        print(all_divs)\n",
    "\n",
    "    with open(\"example.html\") as fp:\n",
    "        soup = BeautifulSoup(fp, 'html.parser')\n",
    "        first_div = soup.find(\"div\")\n",
    "        print(first_div)\n",
    "        \n",
    "2. 태그와 속성을 이용하기 find(), find_all()       \n",
    "with open(\"example.html\") as fp:\n",
    "    soup = BeautifulSoup(fp, 'html.parser')\n",
    "    ex_id_divs = soup.find('div', {'id' : 'ex_id'})\n",
    "    print(ex_id_divs)\n",
    "    \n",
    "3. select() select_one, find() 차이 => accepts CSS selectors, find() does not \n",
    "   select() 리턴 list, select_one 리턴 None\n",
    "    from bs4 import BeautifulSoup\n",
    "    from glob import iglob\n",
    "\n",
    "    def parse_find(soup):\n",
    "        author = soup.find(\"h4\", class_=\"h12 talk-link__speaker\").text\n",
    "        title = soup.find(\"h4\", class_=\"h9 m5\").text\n",
    "        date = soup.find(\"span\", class_=\"meta__val\").text.strip()\n",
    "        soup.find(\"footer\",class_=\"footer\").find_previous(\"data\", {\n",
    "            \"class\": \"talk-transcript__para__time\"}).text.split(\":\")\n",
    "        soup.find_all(\"span\",class_=\"talk-transcript__fragment\")\n",
    "\n",
    "\n",
    "\n",
    "    def parse_select(soup): # 시간도 짧다.\n",
    "        author = soup.select_one(\"h4.h12.talk-link__speaker\").text\n",
    "        title = soup.select_one(\"h4.h9.m5\").text\n",
    "        date = soup.select_one(\"span.meta__val\").text.strip()\n",
    "        soup.select_one(\"footer.footer\").find_previous(\"data\", {\n",
    "            \"class\": \"talk-transcript__para__time\"}).text\n",
    "        soup.select(\"span.talk-transcript__fragment\")\n",
    "\n",
    "\n",
    "    def  test(patt, func):\n",
    "        for html in iglob(patt):\n",
    "            with open(html) as f:\n",
    "                func(BeautifulSoup(f, \"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 1 개의 페이지가 확인 됬습니다.\n",
      "\n",
      "[토요워치] 도심 속 'P'튀기는 전쟁\n",
      "\n",
      "서울 아파트 매매수요 '겨울잠'…매매우위지수 계속 떨어진다\n",
      "\n",
      "입주 앞둔 서울 아파트, 전셋값도 분양가 추월\n",
      "\n",
      "이달 집들이 서울 아파트, 전셋값도 분양가 뛰어넘어\n",
      "\n",
      "KTX역세권 개발 따라 집값 오른다…속초·송도 ‘주목’\n",
      "\n",
      "푹 꺼진 제주 부동산… “반전도 어려워”\n",
      "\n",
      "서울 고가주택 위주 매매價 ↓· 전세價 ↑\n",
      "\n",
      "서울 A급 오피스 공실률 여의도↓·강남↑… 이유는?\n",
      "\n",
      "다주택 투자 수요 꽁꽁… 똘똘한 한채 집중?\n",
      "1인 기업 시대 도래…오피스 시장서 전용 40㎡ 모듈형 평면 인기\n",
      "\n",
      "학군불패·재건축 호재…들뜬 ‘목동’\n",
      "\n",
      "부동산 규제에 코로나까지…\"2월 주택사업 어려움 지속\"\n",
      "\n",
      "15억 넘보던 강북 새 아파트 규제에 뒷걸음질…\n",
      "\n",
      "[집코노미] \"월세 받아 종부세 내자\"…강남 반전세값 상승률 역대 최고\n",
      "\n",
      "LH 아파트 용지 없어서 못판다…지난해 3조7천억원어치 매각\n",
      "\n",
      "서울 전세난 우려… 집주인 우위계약 '준전세' 증가\n",
      "\n",
      "[르포] 속도 내는 목동 재건축 분위기는?\n",
      "\n",
      "\"늘어난 세금 세입자에게 전가\"…서울 아파트 '반전세' 유행\n",
      "\n",
      "[재건축 조합장의 세계] ③조합장 바꾸니 1년반만에 철거 완료… \"평당 1억을 만든건 리더십\"\n",
      "‘풍선효과’ 수원-용인 집값 4주 연속 상승세\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "maximum = 0\n",
    "page = 1\n",
    "\n",
    "URL = 'http://land.naver.com/news/field.nhn?page=1'\n",
    "response = requests.get(URL)\n",
    "source = response.text\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "while 1:\n",
    "\tpage_list = soup.findAll(\"a\", {\"class\": \"NP=r:\" + str(page)}) # \"NP=r:%s\" % page  \n",
    "    #copy element => <a href=\"javascript:goPage(1)\" class=\"NP=r:1\" style=\"margin-right:0px; padding:0px 0px 0px 0px; background:#fff;\"><strong>1</strong></a>\n",
    "    #copy selector => content > div.paginate.paginate_line.NEI\\=a\\:lst\\.page > a\n",
    "\tif not page_list: # NP=r:숫자가 없다는 것은 초과했다는 뜻\n",
    "\t\tmaximum = page - 1 # 초과1개했으므로 -1\n",
    "\t\tbreak\n",
    "\tpage = page + 1\n",
    "print(\"총 \" + str(maximum) + \" 개의 페이지가 확인 됬습니다.\")\n",
    "\n",
    "whole_source = \"\"\n",
    "for page_number in range(1, maximum+1):\n",
    "\tURL = 'http://land.naver.com/news/field.nhn?page=' + str(page_number)\n",
    "\tresponse = requests.get(URL)\n",
    "\twhole_source = whole_source + response.text\n",
    "soup = BeautifulSoup(whole_source, 'html.parser')\n",
    "find_title = soup.select(\"#content > div.section_headline > ul > li > dl > dt > a\") # 뉴스제목의 선택자\n",
    "                          #copy selector => content > div.section_headline > ul 이런식으로 \n",
    "\n",
    "for title in find_title:\n",
    "    print(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup 예제\n",
    "test_url = \"https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=136990&type=after&page=1\"\n",
    "resp = requests.get(test_url)\n",
    "html = BeautifulSoup(resp.content, 'html.parser')\n",
    "html\n",
    "\n",
    "# BeautifulSoup 메서드 find, select\n",
    "score_result = html.find('div', {'class': 'score_result'})\n",
    "lis = score_result.findAll('li')\n",
    "lis[0]\n",
    "\n",
    "review_text = lis[0].find('p').getText()\n",
    "review_text\n",
    "\n",
    "score = lis[0].find('em').getText()\n",
    "score\n",
    "\n",
    "# 좋아요 싫어요\n",
    "like = lis[0].find('div', {'class': 'btn_area'}).findAll('span')[1].getText()\n",
    "dislike = lis[0].find('div', {'class': 'btn_area'}).findAll('span')[3].getText()\n",
    "like, dislike\n",
    "\n",
    "nickname = lis[0].findAll('a')[0].find('span').getText()\n",
    "nickname\n",
    "\n",
    "# 작성일\n",
    "from datetime import datetime\n",
    "created_at = datetime.strptime(li.find('dt').findAll('em')[-1].getText(), \"%Y.%m.%d %H:%M\")\n",
    "created_at\n",
    "\n",
    "# 전체 댓글수\n",
    "result = html.find('div', {'class':'score_total'}).find('strong').findChildren('em')[1].getText()\n",
    "int(result.replace(',', ''))\n",
    "\n",
    "# 함수\n",
    "def get_data(url):\n",
    "    resp = requests.get(url)\n",
    "    html = BeautifulSoup(resp.content, 'html.parser')\n",
    "    score_result = html.find('div', {'class': 'score_result'})\n",
    "    lis = score_result.findAll('li')\n",
    "    for li in lis:\n",
    "        nickname = li.findAll('a')[0].find('span').getText()\n",
    "        created_at = datetime.strptime(li.find('dt').findAll('em')[-1].getText(), \"%Y.%m.%d %H:%M\")\n",
    "\n",
    "        review_text = li.find('p').getText()\n",
    "        score = li.find('em').getText()\n",
    "        btn_likes = li.find('div', {'class': 'btn_area'}).findAll('span')\n",
    "        like = btn_likes[1].getText()\n",
    "        dislike = btn_likes[3].getText()\n",
    "\n",
    "        watch_movie = li.find('span', {'class':'ico_viewer'})\n",
    "\n",
    "        # 간단하게 프린트만 했습니다.\n",
    "        print(nickname, review_text, score, like, dislike, created_at, watch_movie and True or False)\n",
    "\n",
    "# 완성\n",
    "test_url = 'https://movie.naver.com/movie/bi/mi/pointWriteFormList.nhn?code=136990&type=after'\n",
    "resp = requests.get(test_url)\n",
    "html = BeautifulSoup(resp.content, 'html.parser')\n",
    "result = html.find('div', {'class':'score_total'}).find('strong').findChildren('em')[1].getText()\n",
    "total_count = int(result.replace(',', ''))\n",
    "\n",
    "for i in range(1, int(total_count / 10) + 1):\n",
    "    url = test_url + '&page=' + str(i)\n",
    "    print('url: \"' + url + '\" is parsing....')\n",
    "    get_data(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 네이버 평점 requests soup\n",
    "* 전체 페이지 soup처리는 되나 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아이가 보고 감동적이라고 또 보자고 하네요\n",
      "아들이랑 영화보러 다닌지 2년째인데 실망했던건 처음 ㅠㅠ중국 겨냥인듯한데 지금 시국에 아 괜히 영화도 미워지네요아들은 재미있게 본거 같아요뭐 공룡 그래픽은 좋아요 볼만해요 나머지는 ㅠㅠ\n",
      "대실망이임 뭔중국인이 주인공이고 미니특공대는 병풍만들고 스토리는산으로가고\n",
      "미니특공대가 조연이에요. 개인적으로 미니특공대가 하는게 없어서 실망했어요\n",
      "좋아요 ㅎㅎ재밌게볼게요ㅔ\n",
      "공룡 + 미니특공대 = 진리\n",
      "한국애니 추천드립니다. 아이들과 함께 보세요\n",
      "아이와 같이 봤는데 진짜 재밌었다고 해요 어른인 제가 봐도 흥미진진했어요 감동과 교훈을 주는 영화예요\n",
      "음 그냥 또 보고 싶은 영화\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['아이가 보고 감동적이라고 또 보자고 하네요',\n",
       " '아들이랑 영화보러 다닌지 2년째인데 실망했던건 처음 ㅠㅠ중국 겨냥인듯한데 지금 시국에 아 괜히 영화도 미워지네요아들은 재미있게 본거 같아요뭐 공룡 그래픽은 좋아요 볼만해요 나머지는 ㅠㅠ',\n",
       " '대실망이임 뭔중국인이 주인공이고 미니특공대는 병풍만들고 스토리는산으로가고',\n",
       " '미니특공대가 조연이에요. 개인적으로 미니특공대가 하는게 없어서 실망했어요',\n",
       " '좋아요 ㅎㅎ재밌게볼게요ㅔ',\n",
       " '공룡 + 미니특공대 = 진리',\n",
       " '한국애니 추천드립니다. 아이들과 함께 보세요',\n",
       " '아이와 같이 봤는데 진짜 재밌었다고 해요 어른인 제가 봐도 흥미진진했어요 감동과 교훈을 주는 영화예요',\n",
       " '음 그냥 또 보고 싶은 영화']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "maximum = 0\n",
    "page = 1\n",
    "\n",
    "# url = \"https://movie.naver.com/movie/point/af/list.nhn?st=mcode&target=after&sword=%s&page=%s\" % (190325, 1)\n",
    "# response = requests.get(url)\n",
    "# source = response.text\n",
    "# soup = BeautifulSoup(source, 'html.parser')\n",
    "# print(soup)\n",
    "\n",
    "# while 1:\n",
    "# \tpage_list = soup.findAll(\"a\", {\"class\": \"NP=r:\" + str(page)}) # \"NP=r:%s\" % page  \n",
    "#     #copy element => <a href=\"javascript:goPage(1)\" class=\"NP=r:1\" style=\"margin-right:0px; padding:0px 0px 0px 0px; background:#fff;\"><strong>1</strong></a>\n",
    "#     #copy selector => content > div.paginate.paginate_line.NEI\\=a\\:lst\\.page > a\n",
    "# \tif not page_list: # NP=r:숫자가 없다는 것은 초과했다는 뜻\n",
    "# \t\tmaximum = page - 1 # 초과1개했으므로 -1\n",
    "# \t\tbreak\n",
    "# \tpage = page + 1\n",
    "# print(\"총 \" + str(maximum) + \" 개의 페이지가 확인 됬습니다.\")\n",
    "\n",
    "# whole_source = \"\"\n",
    "import re\n",
    "review=[]\n",
    "for page_number in range(1, 2):\n",
    "    URL = \"https://movie.naver.com/movie/point/af/list.nhn?st=mcode&target=after&sword=%s&page=%s\" % (190325, page_number)\n",
    "    response = requests.get(URL)\n",
    "    source = response.text \n",
    "    soup = BeautifulSoup(re.sub(\"&#(?![0-9])\",\"\",source), 'html.parser') # fText(soup.select('.class tag'))\n",
    "    #print(soup) ok\n",
    "    #r = soup.select('#old_content') #ok\n",
    "    rs = soup.select('#old_content > table')[0].find('tbody').findAll('tr') # ok # table.list_netizen # [0].get('onclick')\n",
    "    # r = soup.select('#old_content > table > tbody') # error \n",
    "                            #select => old_content > table > tbody > tr:nth-child(1) > td.title > div\n",
    "                            #xpath => //*[@id=\"old_content\"]/table/tbody/tr[1]/td[2]/text()\n",
    "    # rone = soup.select_one('#old_content > table >tbody')  # None\n",
    "    # r_body=r[0].find('tbody') # ok\n",
    "    # r_ = r[0].find('tbody').find('tr') # find가 첫번째만 리턴하므로 page number마다 첫 평점만 리턴한다 => findAll\n",
    "    #r=r[0].find('tbody').findAll('tr') # findAll # page마다 모든 tr 10ea 리턴\n",
    "    #r=r[0].find('td', {'class' : 'title'})\n",
    "    #print(rs)\n",
    "#     print(r)\n",
    "#     rs = r[0].select('tr:nth-child(1)') #old_content > table > tbody > tr:nth-child(1)\n",
    "    if not len(rs): break\n",
    "# #     p = re.compile('<br\\/>.*', re.MULTILINE)\n",
    "# #     rs = p.finditer(str(r[0]))\n",
    "    for r in rs:\n",
    "        r_=re.search('<br\\/>.*', str(r)).group().replace('<br/>','').strip()\n",
    "        if (r_==''): continue\n",
    "        print(r_)\n",
    "        review.append(r_)\n",
    "review\n",
    "#     for p in ps:\n",
    "#         p.replace('<br/>', '').strip()\n",
    "#         review.append(p)\n",
    "    # r=re.search('(?<=(<br/>+)\".\"', r[0]) # error\n",
    "    #r=re.match('<br/>.*', str(r[0])).group() # error #'NoneType' object has no attribute 'group'\n",
    "#     for r in rs:\n",
    "#         r_=p.search('<br\\/>.*', r).group().replace('<br/>', '').strip() # re.sub()\n",
    "#         print(r_)\n",
    "#         review.append(r_)\n",
    "    #r_list=r\n",
    "\n",
    "    \n",
    "    # match()\t문자열의 처음부터 정규식과 매치되는지 조사한다.\n",
    "    # search()\t문자열 전체를 검색하여 정규식과 매치되는지 조사한다.\n",
    "    # findall()\t정규식과 매치되는 모든 문자열(substring)을 리스트로 돌려준다.\n",
    "    # finditer()\t정규식과 매치되는 모든 문자열(substring)을 반복 가능한 객체로 돌려준다.\n",
    "    # match(), search() 리턴된 객체의 메서드\n",
    "        #    group()\t매치된 문자열을 돌려준다.\n",
    "        #    start()\t매치된 문자열의 시작 위치를 돌려준다.\n",
    "        #    end()\t매치된 문자열의 끝 위치를 돌려준다.\n",
    "        #    span()\t매치된 문자열의 (시작, 끝)에 해당하는 튜플을 돌려준다.\n",
    "                \n",
    "#     p = re.compile(정규표현식)\n",
    "#     m = p.match( 'string goes here' )\n",
    "#     if m:\n",
    "#         print('Match found: ', m.group())\n",
    "#     else:\n",
    "#         print('No match')\n",
    "\n",
    "# >>> result = p.findall(\"life is too short\")\n",
    "# >>> print(result)\n",
    "# ['life', 'is', 'too', 'short']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## selenium chrome webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time   # time.sleep(1) 사용하기 위해 1초동안 \n",
    "\n",
    "# 제어 브라우저 작동(윈도우)\n",
    "# driver=webdriver.Chrome(\"chromedriver\")\n",
    "# time.sleep(1)\n",
    "\n",
    "# 제어 브라우저 작동(리눅스)\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "time.sleep(1)\n",
    "\n",
    "chrome_options=webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "#chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "driver= webdriver.Chrome('/usr/bin/chromedriver', options=chrome_options)\n",
    "#yield driver\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "\n",
      "[토요워치] 도심 속 'P'튀기는 전쟁\n",
      "\n",
      "서울 아파트 매매수요 '겨울잠'…매매우위지수 계속 떨어진다\n",
      "\n",
      "입주 앞둔 서울 아파트, 전셋값도 분양가 추월\n",
      "\n",
      "이달 집들이 서울 아파트, 전셋값도 분양가 뛰어넘어\n",
      "\n",
      "KTX역세권 개발 따라 집값 오른다…속초·송도 ‘주목’\n",
      "\n",
      "푹 꺼진 제주 부동산… “반전도 어려워”\n",
      "\n",
      "서울 고가주택 위주 매매價 ↓· 전세價 ↑\n",
      "\n",
      "서울 A급 오피스 공실률 여의도↓·강남↑… 이유는?\n",
      "\n",
      "다주택 투자 수요 꽁꽁… 똘똘한 한채 집중?\n",
      "1인 기업 시대 도래…오피스 시장서 전용 40㎡ 모듈형 평면 인기\n",
      "\n",
      "학군불패·재건축 호재…들뜬 ‘목동’\n",
      "\n",
      "부동산 규제에 코로나까지…\"2월 주택사업 어려움 지속\"\n",
      "\n",
      "15억 넘보던 강북 새 아파트 규제에 뒷걸음질…\n",
      "\n",
      "[집코노미] \"월세 받아 종부세 내자\"…강남 반전세값 상승률 역대 최고\n",
      "\n",
      "LH 아파트 용지 없어서 못판다…지난해 3조7천억원어치 매각\n",
      "\n",
      "서울 전세난 우려… 집주인 우위계약 '준전세' 증가\n",
      "\n",
      "[르포] 속도 내는 목동 재건축 분위기는?\n",
      "\n",
      "\"늘어난 세금 세입자에게 전가\"…서울 아파트 '반전세' 유행\n",
      "\n",
      "[재건축 조합장의 세계] ③조합장 바꾸니 1년반만에 철거 완료… \"평당 1억을 만든건 리더십\"\n",
      "‘풍선효과’ 수원-용인 집값 4주 연속 상승세\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "maximum = 0\n",
    "page = 1\n",
    "\n",
    "URL = 'http://land.naver.com/news/field.nhn?page=1'\n",
    "driver.get(URL)\n",
    "\n",
    "div_elems = driver.find_elements_by_xpath('//*[@id=\"content\"]/div[2]') # \"xpath\" => 'xpath' # 복수경로 |  # list 리턴\n",
    "#print(div_elems) # [<selenium.webdriver.remote.webelement.WebElement (session=\"81c6f0fff217f64ed59861be91d2bd81\", element=\"0.9683018106275785-1\")>]\n",
    "#print (div_elems) \n",
    "    \n",
    "#print (div_elems.get_attribute('text'))  # error (because list) #head 태그 안에 있는 title정보는 get_attribute('text')메서드로 가져올 수 있다.\n",
    "print (div_elems[0].get_attribute('text')) # None \n",
    "#soup = BeautifulSoup(div_elems) # error\n",
    "\n",
    "#print (div_elems[0].text) # html코드 제외하고 body 안에 있는 태그 요소는 .text 로 추출할 수 있습니다. (출력이 잘 안되면, 둘다 써보셔도 좋습니다.)\n",
    "\n",
    "for a in div_elems[0].find_elements_by_tag_name('a'): # tag_name => 'p', 'a' \n",
    "    print (a.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find_elements_by_class_name, find_elements_by_tag_name\n",
    "# driver.find_elements_by_class_name, find_elements_by_tag_name  ==> 다른 하위에서도 class name 중복될 수 있음 ==> xpath\n",
    "\n",
    "/ : 루트 node로부터 절대경로를 나타냄\n",
    "// : 현재 Node로부터 문서상의 모든 Node 조회\n",
    ".  : 현재 node 선택 \n",
    ".. : 현재 node의 부모 node 선택\n",
    "@  : 현재 node의 속성 선택\n",
    " \n",
    "[...]\n",
    "*  : 매칭되는 모든  elementNode\n",
    "@* : 매칭되는 모든 속성 Node\n",
    "    \n",
    "//@href : href 속성이 있는 모든 태그 선택\n",
    "//a[@href='http://google.com'] : a 태그의 href 속성에 http://google.com 속성값을 가진 모든 태그 선택\n",
    "(//a)[3] : 문서의 세 번째 링크 선택\n",
    "(//table)[last()] : 문서의 마지막 테이블 선택\n",
    "(//a)[position() < 3] : 문서의 처음 두 링크 선택\n",
    "//table/tr/* 모든 테이블에서 모든 자식 tr 태그 선택\n",
    "//div[@*] 속성이 하나라도 있는 div 태그 선택\n",
    "//div[@class='  '/pre[@class=' ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# html = div_elems[0].page_source    \n",
    "# soup = BeautifulSoup(html, \"html.parser\")\n",
    "# ===> 'WebElement' object has no attribute 'page_source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### selenium chrome driver 소스코드 전체파싱\n",
    "# driver. ,,, 형태로 찾아가다가, 변수로 저장이 필요한 부분은 변수로 저장\n",
    "# html = driver.page_source  <== driver 현재 페이지를 전체 html로 받아서 soup 넣기\n",
    "# soup = BeautifulSoup(html, \"html.parser\")   \n",
    "        \n",
    "# driver.get(\"http://info.nec.go.kr/main/showDocument.xhtml?electionId=0000000000&topMenuId=VC&secondMenuId=VCCP09\")\n",
    "\n",
    "# #대통령선거 id, click\n",
    "# driver.find_element_by_id(\"electionType1\").click()\n",
    "\n",
    "# # 제19대 선택 id, send_keys\n",
    "# driver.find_element_by_id(\"electionName\").send_keys(\"제19대\")\n",
    "\n",
    "# # 대통령선거 선택 id, send_keys\n",
    "# driver.find_element_by_id(\"electionCode\").send_keys(\"대통령선거\")\n",
    "\n",
    "# # 시도이름 선택 select id  ============================================> 변수로 받기\n",
    "# sido_list_raw = driver.find_element_by_id(\"cityCode\")\n",
    "\n",
    "# # 시도이름 선택 option tag\n",
    "# sido_list = sido_list_raw.find_elements_by_tag_name(\"option\")\n",
    "# time.sleep(1)\n",
    "\n",
    "# # 시도이름 선택 select id =============================================> 다른 변수로 받기\n",
    "# element = driver.find_element_by_id(\"cityCode\")\n",
    "# element.send_keys(each_sido)\n",
    "\n",
    "# #검색버튼 클릭 click\n",
    "# driver.find_element_by_id(\"searchBtn\").click()\n",
    "\n",
    "# # 소스코드 전체파싱 ==========================================> driver가 현재 가르키고 있는 page \n",
    "# html = driver.page_source\n",
    "\n",
    "# # 소스코드 soup 넣기 \n",
    "# soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "# # 테이블 soup에 넣기 ==========================================> soup 객체를 변수로 받아서 \n",
    "# table = soup.find(id=\"table01\")\n",
    "\n",
    "# # df 문자열로 바뀐 상태에서 문자열만 추출해 내어 데이터프레임형태로 list에 저장  =====================> df 받기 ( read_html )  \n",
    "# df=pd.read_html(str(table))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### naver 평점 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import time   # time.sleep(1) 사용하기 위해 1초동안 \n",
    "\n",
    "# 제어 브라우저 작동(윈도우)\n",
    "# driver=webdriver.Chrome(\"chromedriver\")\n",
    "# time.sleep(1)\n",
    "\n",
    "# 제어 브라우저 작동(리눅스)\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "time.sleep(1)\n",
    "\n",
    "chrome_options=webdriver.ChromeOptions()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "#chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "driver= webdriver.Chrome('/usr/bin/chromedriver', options=chrome_options)\n",
    "#yield driver\n",
    "time.sleep(1)\n",
    "\n",
    "url = \"https://movie.naver.com/movie/point/af/list.nhn?st=mcode&target=after&sword=%s&page=%s\" % (190325, 1)\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnexpectedAlertPresentException",
     "evalue": "Alert Text: None\nMessage: unexpected alert open: {Alert text : 리눅스에서 지원되지 않는 브라우저로 서비스 이용에 제한이 있습니다.}\n  (Session info: headless chrome=80.0.3987.87)\n  (Driver info: chromedriver=2.41.578700 (2f1ed5f9343c13f73144538f15c00b370eda6706),platform=Linux 5.3.0-28-generic x86_64)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mUnexpectedAlertPresentException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ec275010b40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# 절대 경로\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mreview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/html/body/div/div[4]/div/div/div/div/div[1]/table/tbody/tr[1]/td[2]/text()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# for a in current_movie[0].find_elements_by_tag_name('option'): # tag_name => 'p', 'a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     print (a.text)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_elements_by_xpath\u001b[0;34m(self, xpath)\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0melements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements_by_xpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//div[contains(@class, 'foo')]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \"\"\"\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfind_element_by_link_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mfind_elements\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         return self.execute(Command.FIND_ELEMENTS, {\n\u001b[1;32m   1006\u001b[0m             \u001b[0;34m'using'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             'value': value})['value'] or []\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/selenium/webdriver/remote/webdriver.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             response['value'] = self._unwrap_value(\n\u001b[1;32m    323\u001b[0m                 response.get('value', None))\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/selenium/webdriver/remote/errorhandler.py\u001b[0m in \u001b[0;36mcheck_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m'alert'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0malert_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'alert'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malert_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mexception_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedAlertPresentException\u001b[0m: Alert Text: None\nMessage: unexpected alert open: {Alert text : 리눅스에서 지원되지 않는 브라우저로 서비스 이용에 제한이 있습니다.}\n  (Session info: headless chrome=80.0.3987.87)\n  (Driver info: chromedriver=2.41.578700 (2f1ed5f9343c13f73144538f15c00b370eda6706),platform=Linux 5.3.0-28-generic x86_64)\n"
     ]
    }
   ],
   "source": [
    "# current_movie=driver.find_elements_by_xpath('//*[@id=\"current_movie\"]')\n",
    "#soup = BeautifulSoup(html, \"html.parser\")    \n",
    "#print(soup)\n",
    "#review = driver.find_elements_by_xpath('//*[@id=\"old_content\"]/table/tbody/tr[1]/td[2]/text()')\n",
    "\n",
    "# 절대 경로\n",
    "review = driver.find_elements_by_xpath('/html/body/div/div[4]/div/div/div/div/div[1]/table/tbody/tr[1]/td[2]/text()')\n",
    "# for a in current_movie[0].find_elements_by_tag_name('option'): # tag_name => 'p', 'a' \n",
    "#     print (a.text)\n",
    "print(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(URL)\n",
    "source = response.text\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "while 1:\n",
    "\tpage_list = soup.findAll(\"a\", {\"class\": \"NP=r:\" + str(page)}) # \"NP=r:%s\" % page  \n",
    "    #copy element => <a href=\"javascript:goPage(1)\" class=\"NP=r:1\" style=\"margin-right:0px; padding:0px 0px 0px 0px; background:#fff;\"><strong>1</strong></a>\n",
    "    #copy selector => content > div.paginate.paginate_line.NEI\\=a\\:lst\\.page > a\n",
    "\tif not page_list: # NP=r:숫자가 없다는 것은 초과했다는 뜻\n",
    "\t\tmaximum = page - 1 # 초과1개했으므로 -1\n",
    "\t\tbreak\n",
    "\tpage = page + 1\n",
    "print(\"총 \" + str(maximum) + \" 개의 페이지가 확인 됬습니다.\")\n",
    "\n",
    "whole_source = \"\"\n",
    "for page_number in range(1, maximum+1):\n",
    "\tURL = 'http://land.naver.com/news/field.nhn?page=' + str(page_number)\n",
    "\tresponse = requests.get(URL)\n",
    "\twhole_source = whole_source + response.text\n",
    "soup = BeautifulSoup(whole_source, 'html.parser')\n",
    "find_title = soup.select(\"#content > div.section_headline > ul > li > dl > dt > a\") # 뉴스제목의 선택자\n",
    "                          #copy selector => content > div.section_headline > ul 이런식으로 \n",
    "\n",
    "for title in find_title:\n",
    "\tprint(title.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
